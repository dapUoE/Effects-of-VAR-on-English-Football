{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access files from this path: \"C:\\Users\\Daniel Price\\Documents\\Documents\\Exeter\\Data_Science_in_Economics\\Empirical_Project\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each file, from 2000-01 to 2021-22 (inclusive), do the following:\n",
    "Convert the csv file to a pandas dataframe\n",
    "Since each year is the same data, but for a different year, we can concatenate the dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Div       Date     HomeTeam  ... MaxCAHA  AvgCAHH  AvgCAHA\n",
      "0    E0 2000-08-19     Charlton  ...     NaN      NaN      NaN\n",
      "1    E0 2000-08-19      Chelsea  ...     NaN      NaN      NaN\n",
      "2    E0 2000-08-19     Coventry  ...     NaN      NaN      NaN\n",
      "3    E0 2000-08-19        Derby  ...     NaN      NaN      NaN\n",
      "4    E0 2000-08-19        Leeds  ...     NaN      NaN      NaN\n",
      "..   ..        ...          ...  ...     ...      ...      ...\n",
      "318  E0 2024-04-13  Bournemouth  ...    1.84     2.08     1.79\n",
      "319  E0 2024-04-14    Liverpool  ...    2.06     1.87     2.00\n",
      "320  E0 2024-04-14     West Ham  ...    1.83     2.10     1.79\n",
      "321  E0 2024-04-14      Arsenal  ...    2.02     1.91     1.95\n",
      "322  E0 2024-04-15      Chelsea  ...    2.09     1.85     2.02\n",
      "\n",
      "[9063 rows x 146 columns]\n",
      "Number of files converted: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to parse dates adaptively\n",
    "def parse_dates(dates):\n",
    "    # Checking the year format based on length and adjusting accordingly\n",
    "    if len(dates[0].split('/')[-1]) == 2:\n",
    "        return pd.to_datetime(dates, format='%d/%m/%y')\n",
    "    elif len(dates[0].split('/')[-1]) == 4:\n",
    "        return pd.to_datetime(dates, format='%d/%m/%Y')\n",
    "    else:\n",
    "        # Fallback to infer the format if unexpected length\n",
    "        return pd.to_datetime(dates, infer_datetime_format=True)\n",
    "\n",
    "# Data directory\n",
    "data = r\"C:\\Users\\Daniel Price\\Documents\\Documents\\Exeter\\Data_Science_in_Economics\\Empirical_Project\\Data\"\n",
    "\n",
    "# Get the list of files in the data directory\n",
    "file_list = os.listdir(data)\n",
    "\n",
    "# Initialize an empty dataframe to store the concatenated data\n",
    "concatenated_df = pd.DataFrame()\n",
    "\n",
    "# Define the pattern for the file names\n",
    "pattern = re.compile(r'^\\d{4}-\\d{2}\\.csv$')\n",
    "\n",
    "# Initialize a counter for the number of files converted\n",
    "file_count = 0\n",
    "\n",
    "# Iterate over each file\n",
    "for file_name in file_list:\n",
    "    # Check if the file matches the pattern for a CSV file\n",
    "    if pattern.match(file_name):\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(data, file_name)\n",
    "        \n",
    "        # Read the CSV file into a dataframe, explicitly parsing the date\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date'], date_parser=parse_dates)\n",
    "        \n",
    "        # Concatenate the dataframe to the existing data\n",
    "        concatenated_df = pd.concat([concatenated_df, df])\n",
    "\n",
    "        # Increment the counter\n",
    "        file_count += 1\n",
    "\n",
    "# Print the concatenated dataframe\n",
    "print(concatenated_df)\n",
    "\n",
    "# Print the number of files converted\n",
    "print(f\"Number of files converted: {file_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAR was implemented after in the 2019/20 season, so we will mark the data which had VAR with a 1, and the data which did not have VAR with a 0. We will make a new column called 'VAR' which will have this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unparseable dates set as NaT: 0\n",
      "    Div       Date     HomeTeam        AwayTeam  ...  MaxCAHA  AvgCAHH AvgCAHA  VAR\n",
      "0    E0 2000-08-19     Charlton        Man City  ...      NaN      NaN     NaN    0\n",
      "1    E0 2000-08-19      Chelsea        West Ham  ...      NaN      NaN     NaN    0\n",
      "2    E0 2000-08-19     Coventry   Middlesbrough  ...      NaN      NaN     NaN    0\n",
      "3    E0 2000-08-19        Derby     Southampton  ...      NaN      NaN     NaN    0\n",
      "4    E0 2000-08-19        Leeds         Everton  ...      NaN      NaN     NaN    0\n",
      "..   ..        ...          ...             ...  ...      ...      ...     ...  ...\n",
      "318  E0 2024-04-13  Bournemouth      Man United  ...     1.84     2.08    1.79    1\n",
      "319  E0 2024-04-14    Liverpool  Crystal Palace  ...     2.06     1.87    2.00    1\n",
      "320  E0 2024-04-14     West Ham          Fulham  ...     1.83     2.10    1.79    1\n",
      "321  E0 2024-04-14      Arsenal     Aston Villa  ...     2.02     1.91    1.95    1\n",
      "322  E0 2024-04-15      Chelsea         Everton  ...     2.09     1.85    2.02    1\n",
      "\n",
      "[9063 rows x 147 columns]\n",
      "Number of matches listed as having VAR: 1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime format, handling unparseable dates by setting them as NaT\n",
    "concatenated_df['Date'] = pd.to_datetime(concatenated_df['Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Identify rows where the date conversion resulted in NaT\n",
    "nat_dates = concatenated_df[concatenated_df['Date'].isna()]\n",
    "\n",
    "# Print the count of NaT entries\n",
    "print(f\"Number of unparseable dates set as NaT: {nat_dates.shape[0]}\")\n",
    "\n",
    "# If there are any NaT values, print them out\n",
    "if not nat_dates.empty:\n",
    "    print(\"List of unparseable dates:\")\n",
    "    print(nat_dates['Date'])  # This will only show NaT, for actual date strings causing issues, you should print another column if available\n",
    "\n",
    "# Define the date for VAR implementation\n",
    "var_start_date = pd.Timestamp('2019-06-01')\n",
    "\n",
    "# Add a 'VAR' column based on the year extracted from the 'date'. VAR implemented from 2019 onwards.\n",
    "concatenated_df['VAR'] = concatenated_df['Date'].apply(lambda x: 1 if x >= var_start_date else 0)\n",
    "\n",
    "# Print the concatenated dataframe with the new 'VAR' column\n",
    "print(concatenated_df)\n",
    "\n",
    "# Print the number of matches listed as having VAR\n",
    "print(f\"Number of matches listed as having VAR: {concatenated_df['VAR'].sum()}\")\n",
    "\n",
    "# Write the concatenated dataframe to a CSV file in the same directory as the datasets\n",
    "data_path = 'your/directory/path'  # Adjust this to your correct path\n",
    "concatenated_df.to_csv(os.path.join(data, 'combined_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns without NaN values: ['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'VAR']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns without NaN values\n",
    "no_nan_columns = concatenated_df.columns[concatenated_df.notna().all()].tolist()\n",
    "\n",
    "print(\"Columns without NaN values:\", no_nan_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal Forest\n",
    "We will use the causal forest model to show the effects of VAR on the number of goals scored in a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "Not all column names are strings. Coercing to strings for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted effect of VAR on the total number of goals scored in the match:  [ 2171.8388942  -3164.37744341     5.10152737  -804.6930611\n",
      "  3210.85307572  -266.39283789  2785.18991734   106.42579257\n",
      "  -199.66917394  1319.90337324]\n",
      "Estimated effect of VAR on the total number of goals scored in the match:  -6.355027773457843\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "# Assuming 'concatenated_df' is your DataFrame and is already loaded\n",
    "# Preprocess Date column\n",
    "concatenated_df['Date'] = pd.to_datetime(concatenated_df['Date'])\n",
    "concatenated_df['Year'] = concatenated_df['Date'].dt.year\n",
    "concatenated_df['Month'] = concatenated_df['Date'].dt.month\n",
    "concatenated_df['Day'] = concatenated_df['Date'].dt.day\n",
    "\n",
    "# Selecting and preparing data\n",
    "features = ['Year', 'Month', 'Day', 'HTHG', 'HTAG',\n",
    "            'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', \n",
    "            'AY', 'HR', 'AR']\n",
    "X = concatenated_df[features]\n",
    "Y = concatenated_df['FTHG'] + concatenated_df['FTAG']\n",
    "T = concatenated_df['VAR']\n",
    "\n",
    "\n",
    "\n",
    "# Define and configure the causal forest model\n",
    "causal_forest = CausalForestDML(\n",
    "    model_y=GradientBoostingRegressor(),  # Model for the outcome\n",
    "    model_t=GradientBoostingClassifier(),  # Model for the treatment\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Fit the causal forest model\n",
    "causal_forest.fit(Y=Y, T=T, X=X)\n",
    "\n",
    "# How much the total number of goals is expected to change due to VAR for each match\n",
    "effects = causal_forest.effect(X)\n",
    "print(\"Predicted effect of VAR on the total number of goals scored in the match: \", effects[:10])\n",
    "\n",
    "# Estimate of VAR's effect on goals scored in a match\n",
    "ate = np.mean(effects)\n",
    "print(\"Estimated effect of VAR on the total number of goals scored in the match: \", ate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    HomeTeam       AwayTeam   VAR_Effect\n",
      "0 2000-08-19    Charlton       Man City  2171.838894\n",
      "1 2000-08-19     Chelsea       West Ham -3164.377443\n",
      "2 2000-08-19    Coventry  Middlesbrough     5.101527\n",
      "3 2000-08-19       Derby    Southampton  -804.693061\n",
      "4 2000-08-19       Leeds        Everton  3210.853076\n",
      "5 2000-08-19   Leicester    Aston Villa  -266.392838\n",
      "6 2000-08-19   Liverpool       Bradford  2785.189917\n",
      "7 2000-08-19  Sunderland        Arsenal   106.425793\n",
      "8 2000-08-19   Tottenham        Ipswich  -199.669174\n",
      "9 2000-08-20  Man United      Newcastle  1319.903373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "# Convert effects to a DataFrame\n",
    "effects_df = pd.DataFrame(effects, columns=['VAR_Effect'])\n",
    "\n",
    "# Concatenate this DataFrame with the original match data DataFrame\n",
    "# Ensure that the indices align correctly\n",
    "concatenated_df['VAR_Effect'] = effects_df['VAR_Effect']\n",
    "\n",
    "# Now, you can see the VAR effect alongside all other match data\n",
    "print(concatenated_df[['Date', 'HomeTeam', 'AwayTeam', 'VAR_Effect']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date     HomeTeam     AwayTeam   VAR_Effect\n",
      "218 2020-01-12  Aston Villa     Man City  7538.839456\n",
      "218 2017-01-22      Chelsea         Hull  7538.839456\n",
      "218 2016-01-17        Stoke      Arsenal  7538.839456\n",
      "218 2021-02-06       Fulham     West Ham  7538.839456\n",
      "218 2006-01-21    Newcastle    Blackburn  7538.839456\n",
      "218 2015-01-18     West Ham         Hull  7538.839456\n",
      "218 2002-01-19        Derby      Ipswich  7538.839456\n",
      "218 2014-01-19      Swansea    Tottenham  7538.839456\n",
      "218 2007-01-13    Blackburn      Arsenal  7538.839456\n",
      "218 2013-01-16      Chelsea  Southampton  7538.839456\n"
     ]
    }
   ],
   "source": [
    "#sorting to find matches with the highest VAR effects\n",
    "sorted_df = concatenated_df.sort_values(by='VAR_Effect', ascending=False)\n",
    "print(sorted_df[['Date', 'HomeTeam', 'AwayTeam', 'VAR_Effect']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Causal Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dowhy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdowhy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CausalModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming 'concatenated_df' is your DataFrame and is already loaded\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Preprocess Date column\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dowhy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dowhy import CausalModel\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Assuming 'concatenated_df' is your DataFrame and is already loaded\n",
    "# Preprocess Date column\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
